{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community langchain-classic\n",
        "!pip install -qU transformers accelerate bitsandbytes sentencepiece gradio\n",
        "!pip install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIy3KmnWsT3q",
        "outputId": "7c201765-d8f0-4a91-a5fd-460a388ebb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/157.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/51.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942d05b2-8c8d-4ca3-b73f-88197afe4254",
        "id": "GJyNkxj7w1m8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# ======================================================\n",
        "# ğŸ”´ UPDATED IMPORTS (LangChain 1.0+)\n",
        "# ======================================================\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_classic.chains import ConversationChain\n",
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# ======================================================\n",
        "# 1ï¸âƒ£ Load LLM\n",
        "# ======================================================\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(\"ğŸ”„ Loading Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"ğŸ”„ Loading Model (4-bit)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"âœ… LLM loaded\")\n",
        "\n",
        "# Define pipeline globally so we can use it directly for 'is_important'\n",
        "generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False  # CRITICAL: Prevents repeating the prompt\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
        "\n",
        "# ======================================================\n",
        "# 2ï¸âƒ£ Embeddings\n",
        "# ======================================================\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# 3ï¸âƒ£ FAISS Persistent Store (FIXED)\n",
        "# ======================================================\n",
        "FAISS_PATH = \"faiss_memory\"\n",
        "\n",
        "if os.path.exists(FAISS_PATH):\n",
        "    vectorstore = FAISS.load_local(\n",
        "        FAISS_PATH,\n",
        "        embeddings,\n",
        "        allow_dangerous_deserialization=True # Required for newer LangChain\n",
        "    )\n",
        "    print(\"âœ… FAISS memory loaded\")\n",
        "else:\n",
        "    # FIX: Use dummy text [\"Initialize\"] instead of [] to avoid dimension errors\n",
        "    vectorstore = FAISS.from_texts([\"Initialize\"], embeddings)\n",
        "    print(\"âœ… FAISS memory created\")\n",
        "\n",
        "# ======================================================\n",
        "# 4ï¸âƒ£ Summary Memory (Short-Term)\n",
        "# ======================================================\n",
        "# FIX: Removed return_messages=True to prevent printing Python objects in chat\n",
        "summary_memory = ConversationSummaryMemory(\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=summary_memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# 5ï¸âƒ£ Memory Importance Scoring (FIXED)\n",
        "# ======================================================\n",
        "def is_important(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Ask the LLM whether a memory is worth storing.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are deciding whether the following interaction is important\n",
        "to remember long-term.\n",
        "\n",
        "Criteria:\n",
        "- Personal preferences\n",
        "- Goals\n",
        "- Decisions\n",
        "- Long-term facts\n",
        "\n",
        "Answer ONLY with yes or no.\n",
        "\n",
        "Interaction:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "    # FIX: Use raw pipeline directly to avoid 'not callable' error\n",
        "    result_list = generation_pipeline(prompt)\n",
        "    result_text = result_list[0]['generated_text'].strip().lower()\n",
        "    return result_text.startswith(\"yes\")\n",
        "\n",
        "# ======================================================\n",
        "# 6ï¸âƒ£ Chat Logic (FIXED)\n",
        "# ======================================================\n",
        "def chat(message, history):\n",
        "    # Retrieve relevant long-term memories\n",
        "    docs = vectorstore.similarity_search(message, k=3)\n",
        "    retrieved_memory = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    augmented_input = f\"\"\"\n",
        "Relevant long-term memory:\n",
        "{retrieved_memory}\n",
        "\n",
        "User message:\n",
        "{message}\n",
        "\"\"\"\n",
        "\n",
        "    # Get response\n",
        "    response = conversation.predict(input=augmented_input)\n",
        "\n",
        "    # FIX: Manually trim hallucinated text (e.g. \"Human:...\")\n",
        "    for stop_string in [\"\\nUser:\", \"\\nHuman:\", \"\\nAI:\"]:\n",
        "        if stop_string in response:\n",
        "            response = response.split(stop_string)[0].strip()\n",
        "\n",
        "    interaction = f\"User: {message}\\nAssistant: {response}\"\n",
        "\n",
        "    # Store only important memories\n",
        "    if is_important(interaction):\n",
        "        vectorstore.add_texts([interaction])\n",
        "        vectorstore.save_local(FAISS_PATH)\n",
        "        print(\"ğŸ§  Important memory saved\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# ======================================================\n",
        "# 7ï¸âƒ£ Gradio UI (FIXED)\n",
        "# ======================================================\n",
        "# FIX: Removed 'theme=\"soft\"' as it causes errors in new Gradio versions\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"ğŸ§  LLM with Smart Long-Term Memory (FAISS)\",\n",
        "    description=\"Summary memory + FAISS + importance filtering (FREE Colab)\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747,
          "referenced_widgets": [
            "5d5c43ffed894aa38421afbdafce7f89",
            "14f710c3be324e8da66223c258d2fec3",
            "2f97dc189d564ae29bce866f2b03ebe6",
            "3d13b11453ca462cae6d108c792412bb",
            "bff74923e4484731b94bdf2032786b13",
            "e50e9e23f3534026a3eda875dd6f1c51",
            "357f4c466d6a4a7fbce034dfc056e0fc",
            "3b787a53bdbc49c9a9666f7a876eaa9f",
            "9ee9a78a12f747168110c780d485b654",
            "80967de2f09a4ea198ed26084363193c",
            "9547e770429c47a6a9ebbed71ca04dbb"
          ]
        },
        "id": "ZUHiYdrFsUmf",
        "outputId": "4eb73ad8-1395-4833-81dd-52a8ac0339ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading Tokenizer...\n",
            "ğŸ”„ Loading Model (4-bit)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d5c43ffed894aa38421afbdafce7f89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LLM loaded\n",
            "âœ… FAISS memory created\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9c5a686fa700617f43.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9c5a686fa700617f43.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}