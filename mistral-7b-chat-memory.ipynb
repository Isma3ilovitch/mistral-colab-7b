{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio transformers accelerate bitsandbytes sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmkLkqkgYXZX",
        "outputId": "d4d199c0-9b95-4564-c11a-f5f1146d648b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXRdfViniTMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ----------------------------\n",
        "# Model loading\n",
        "# ----------------------------\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# ----------------------------\n",
        "# Memory configuration\n",
        "# ----------------------------\n",
        "MAX_HISTORY_TURNS = 5  # keep last N user+assistant pairs\n",
        "\n",
        "def format_history(history):\n",
        "    \"\"\"\n",
        "    Convert chat history into a prompt-friendly format\n",
        "    \"\"\"\n",
        "    formatted = \"\"\n",
        "    for user_msg, bot_msg in history[-MAX_HISTORY_TURNS:]:\n",
        "        formatted += f\"User: {user_msg}\\nAssistant: {bot_msg}\\n\"\n",
        "    return formatted.strip()\n",
        "\n",
        "# ----------------------------\n",
        "# Chat function with memory\n",
        "# ----------------------------\n",
        "def chat_with_memory(message, history):\n",
        "    \"\"\"\n",
        "    message: current user input\n",
        "    history: list of (user, assistant) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    conversation_history = format_history(history)\n",
        "\n",
        "    prompt = f\"\"\"[INST]\n",
        "You are a helpful, clear, and concise AI assistant.\n",
        "\n",
        "Conversation so far:\n",
        "{conversation_history}\n",
        "\n",
        "User: {message}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "# ----------------------------\n",
        "# Gradio UI\n",
        "# ----------------------------\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_memory,\n",
        "    title=\"üß† Open-Source LLM Chat with Memory (Mistral 7B)\",\n",
        "    description=\"7B LLM + Conversation Memory running on FREE Google Colab GPU\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3112b414cca24a72bb17fc2cf2513d48",
            "aca730e31f1d4da29bd02c9cb84b70c2",
            "00803fca3e0142a69cf605ba1aacc108",
            "e5f954323f6540e8ae97370c7c280e08",
            "0af084b65a6f40dbbaae2f344db688a8",
            "f1f305a5c1d444998b5bf328c529ca5f",
            "ef587580515340019373e1e10e625525",
            "940a0cc3a09b49b5bdb0ca854ddd5228",
            "7e1d3dfa47d24886b46cdd356e03e1ae",
            "88c226a8f1624e7099ccb96cdd52cca0",
            "70ee8bce5a0d472488e8aecb7c4e4519",
            "f3a93c77afb24120b1fb0978ab2a6342",
            "adb738cfa75c43c58e9f147cc75f0402",
            "2f5bec1e85554a5fa925bf54477bbae3",
            "12145b9cdc9944a89e54f1c577bf9a5d",
            "e0391115a4524229a46170e7375271bd",
            "b36580dd0d734a88b339e7ba982cd6d1",
            "f80fe579ba824345ac6fcf229fb0c8ff",
            "c3419cfd4e2c4e899daf06632e98f6dc",
            "634d9e8edb2f4e7da54702ecce07ebe3",
            "60d3d40dee03434a9655081183aba931",
            "d9d0f72eef5a47d39f6082c56e9dcf8c",
            "f636adcf416c4e41b6a6d74575dfcf42",
            "87d47ea1431445f0a67e39410d8aff76",
            "82c36d3d830a4790b302e3b398cd3b18",
            "58ca152e6fe04d01a328ac2af790e88f",
            "e9e1c7f6553944c9be32b355a0eb35d6",
            "2486620cb34e4088b59deabc845a0928",
            "d4236499790b4bde8df722539bac15e7",
            "b62ce5419caa4d40b052e67971bd6c29",
            "332df4d56cf248198462cb37e8281f43",
            "f65800278b9d49a7967017a953058e86",
            "7ab0320b38e547bbb15012f4bca1ccb6",
            "4e12640c4599415ea2869f0678b87f4e",
            "495a9fe310c941b7a0dbc539dc6f020c",
            "4e701da85bdd44a2b429c5fb3572f4b6",
            "2a937886151e49beaf5bba80dd29255e",
            "65b1b11dfd584a21a827400f74b5863b",
            "171a6f4fc19742a4a7ee0332654cf7e7",
            "73ab371f76b34b4f90afa03b467654b7",
            "468ad010b4384e1bba338fb4e11d4a58",
            "e71aec9d6c884868bafffb604b0e0535",
            "c713a860ae1a4bcb855652387adff0bb",
            "afc33eeb6084485981cc8267e8a2bfd8",
            "3044e92d82e546aba3594e5fb412e0f6",
            "9c5b26cb779f4777bfa06e6e4699b377",
            "88c2c5e254af4710b8301f86745dd15f",
            "f9d5682278534161aadbb5f0041610c7",
            "ff0788c62ff14bef8821148e5219691a",
            "542caff925d7460d9720c86f3bf4161f",
            "ff3a254bb84f4235afe8f52a2e124fc4",
            "46c1979d7782463faa4c42bdf93d2982",
            "893849e61bcd4adba8a737913cba3633",
            "f6e3c0da5e704a54ad8271349a08d914",
            "23309c23fce348798c42c83745cf621e",
            "b8ea2723ae6d42a299ef00c43b7bac46",
            "01c3a5de73bd4c769f502f4bbe7c17f5",
            "27474cecbaf7458e8a1b5aa0138e4819",
            "d68a30e9a95c4fa8a97036df9d6605e1",
            "d23d5c38fcf04578969c15e1c356da76",
            "918c2ab7a5434b4d8b435d13d06ce7e1",
            "ea2201faafcb4f208513efac12c8df7c",
            "3f4a28bb03194ca292c9e1c5b85f6ce0",
            "3944b5d4e42b40b68169fd9cdc9e1b24",
            "517bbc5fb5f64799a59e7d4700efe9cd",
            "52a6a335d00948ab9aa0d0c9196dfc38",
            "ff1beff8cee74ba4add5c4b57033a303",
            "fd5b3e7d42cd4521a3e20572220e5b2b",
            "29d070931f2342f4ac6220d92361c98e",
            "3d96967b7d314dc5abc60d4114faef8c",
            "c3721331e60a448aa6b2f920c066fc6d",
            "29b74aeb5fe2416182a1351664dcfa93",
            "c1b373157371416898bdc3e72c972481",
            "d1fe3f7bbdba4ca2b458ee8b9426d5b6",
            "094c6e5e290f4745ab77e4a38acf29cc",
            "ec88bf17297640bf969158e59b9a53b9",
            "37e1925cec9540d2a013ef62b8ad15f6",
            "d0e190c45d474e0e8dd53fa32e9178fe",
            "2e8cd643b9d1411499250c5719983805",
            "588a073f8c334cc6a6a12d6c97c1c20a",
            "7de950d66d0d4817a01db80ca44de38d",
            "754547da35054551939fd7baa55a9027",
            "5e3103b5d676453485d77049bc942040",
            "834bb1d45e5d4347bd0ed1d43e946fd5",
            "cff7bdc5aa644e29a13531588c175432",
            "caec58a7586e47f88e8b7940abd1e729",
            "02e0dd74a19b4791badd60d8c5ab82a5",
            "ee6b89faee514355a8c8ef4d4d84581c",
            "e2a12ac826044c5794481a9bfcb9d07f",
            "c1271e8e428540f483d51b3b697f3fec",
            "92fbbfc0e6b341cfa31fe1c3bb3dc85a",
            "91a33ef555db403bb0883c6c29123d59",
            "880dc0912c6e441f9f825911c0604c49",
            "8b353d25e2ba466e8131d42d2c438a5c",
            "1e48c0b3f7fa4fcbb3b978edfd543757",
            "04bd3384ff784f849821440e5f5f1311",
            "8723cc819c894462986e12345bb86144",
            "c3757e8c3c024dc286378ce0db4db32c",
            "3ca10e0bc7ab4b59b99008f979a50ca3",
            "bb1716468bbd43c59ac982c59b3f766c",
            "8e59b01397e44235891d9475b369ed27",
            "3409aa90ccca4bf8b9438c3be70e9a6c",
            "80f158500ff24aff86b2b935032f7a64",
            "59a3abd4a36f4507a440c9971236be49",
            "d8fb3304ddb7490e87086eca52096817",
            "dc63b336089c4bd2974aab093f1a0339",
            "deae97100061461a92b1c4574f20ec6f",
            "74e365257da149349891f76f0487e3e5",
            "d8429811eece449ebe83033ac5a468d0",
            "1e3d210191c441118264104a531ba590",
            "b80790cc1a6048f5b9ff50ffb950aeab",
            "1e1b3cdd8b50495685e1a64d23b47226",
            "3e5fd745fa454923bb90b23cf4ca007c",
            "d9394ed87cb6482db8ff0892f86d831f",
            "67c8836b691242248278c4e4e02952d3",
            "14e83ca2cb33442ea42a03724b86d773",
            "e160d973357844b9b21899c15503944e",
            "8927c6f270554ac48b9d0a1a04257473",
            "a9509c7e76c4457785096c7963e622a4",
            "b729c06b2c6a4e61b44ea8e6e4a01e38",
            "4acb03bf4c904dbeb11dc62c03092df2",
            "60bff3b0a85e4e848c1610503fa2957f",
            "3caeca1800a44c5693ee8c5938299598",
            "c7954675a3764fb3905a569be8d87568",
            "a63357bd974b42cbaee3096828c42ef1",
            "e400b8caad3f42a6b3b44a9576c79146",
            "670a22b8a7264d7ea879cfbdaeaadbea",
            "2871524b24174bf9a0a64599520f93a8",
            "a79980201f2a4fc18eda394691bca960",
            "2e6905337e8241c0905760bc5646a185",
            "81b9b9bf184a41bab57f4ac9f46a9c48",
            "0d101605a35a476697a2915115faf106"
          ]
        },
        "id": "PyGtZjyNYY8N",
        "outputId": "77f48423-6452-4aa7-ff1f-61156f6d9e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3112b414cca24a72bb17fc2cf2513d48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3a93c77afb24120b1fb0978ab2a6342"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f636adcf416c4e41b6a6d74575dfcf42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e12640c4599415ea2869f0678b87f4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3044e92d82e546aba3594e5fb412e0f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8ea2723ae6d42a299ef00c43b7bac46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff1beff8cee74ba4add5c4b57033a303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0e190c45d474e0e8dd53fa32e9178fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2a12ac826044c5794481a9bfcb9d07f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb1716468bbd43c59ac982c59b3f766c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b80790cc1a6048f5b9ff50ffb950aeab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60bff3b0a85e4e848c1610503fa2957f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6353b281d5157857d0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6353b281d5157857d0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}
