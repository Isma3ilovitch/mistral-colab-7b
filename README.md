### mistral-colab-7b
A simple and efficient script to run Mistral-7B-Instruct-v0.2 locally using Hugging Face Transformers. This implementation utilizes 4-bit quantization via bitsandbytes to significantly reduce memory usage, allowing the model to run on consumer-grade GPUs (like Google Colab or local RTX cards).
---

### Mistral 7B + Gradio running in Google Colab with 4-bit quantization
This code demonstrates how to run the Mistral-7B-Instruct-v0.2 model efficiently on Google Colab using 4-bit quantization. It includes scripts for both a basic text generation example and an interactive Gradio chat interface, allowing users to easily chat with the 7-billion-parameter model on a free GPU without needing high-end hardware.
---
### ğŸ§  Mistral Chat with Memory (7B)

A simple **open-source chat application** powered by **Mistral-7B-Instruct**, featuring
conversation **memory**, **4-bit quantization**, and a **Gradio UI** â€” all runnable on a
**FREE Google Colab GPU**.

This project is designed for learning and experimentation with real LLM systems.

---

## âœ¨ Features

- ğŸ§  Short-term conversation memory (last N turns)
- âš¡ 4-bit model loading (low VRAM usage)
- ğŸ›ï¸ Clean chat interface with Gradio
- ğŸš€ Runs on free Google Colab GPU
- ğŸ”“ Fully open-source and easy to modify

---

## ğŸ› ï¸ Tech Stack

- Python
- PyTorch
- Hugging Face Transformers
- Mistral-7B-Instruct
- Gradio
- BitsAndBytes (4-bit quantization)

---

## ğŸš€ Run on Google Colab

1. Open a new **Google Colab** notebook
2. Enable GPU:  
   `Runtime â†’ Change runtime type â†’ GPU`
3. Install dependencies:

```bash
pip install torch transformers gradio bitsandbytes accelerate
```
---

### ğŸ§  Mistral Hybrid Memory Chat (7B)

An open-source **LLM chat system with real memory** powered by **Mistral-7B**,
combining **summary-based context** and **persistent vector memory** using
LangChain and Chroma â€” all runnable on a **FREE Google Colab GPU**.

This project demonstrates how modern AI assistants can remember information
across conversations, not just within a single prompt.

---

## âœ¨ Key Features

- ğŸ§  **ConversationSummaryMemory** (compressed short-term context)
- ğŸ—‚ï¸ **Persistent long-term memory** with Chroma vector database
- ğŸ” Semantic memory retrieval using embeddings
- âš¡ 4-bit quantized Mistral 7B (low VRAM)
- ğŸ›ï¸ Interactive Gradio chat UI
- ğŸ’¾ Memory persists across sessions

---

## ğŸ§© Memory Architecture

This system uses **hybrid memory**:

1. **Summary Memory**
   - Keeps conversations short and efficient
   - Prevents context window overflow

2. **Vector Memory (Chroma)**
   - Stores past conversations as embeddings
   - Retrieves relevant memories by similarity
   - Persists on disk

Together, this mimics **human-like long-term memory**.

---

## ğŸ› ï¸ Tech Stack

- Python
- PyTorch
- Hugging Face Transformers
- Mistral-7B-Instruct
- LangChain (classic + community)
- Chroma Vector Database
- Sentence-Transformers
- Gradio

---

## ğŸš€ Run on Google Colab

1. Open a new **Google Colab** notebook  
2. Enable GPU:
   `Runtime â†’ Change runtime type â†’ GPU`

3. Install dependencies:

```bash
pip install torch transformers gradio accelerate bitsandbytes \
langchain langchain-community sentence-transformers chromadb
```


---
# ğŸ§  LLM with Smart Long-Term Memory (Mistral 7B + FAISS)

A minimal **LLM chat system with selective long-term memory**, built using
**Mistral-7B**, **LangChain**, and **FAISS**, runnable on a **FREE Google Colab GPU**.

Instead of storing every message, this project uses the LLM itself to decide  
**what is worth remembering long-term**.

---

## âœ¨ Features

- ğŸ§  Short-term memory via `ConversationSummaryMemory`
- ğŸ—‚ï¸ Persistent long-term memory with **FAISS**
- âš–ï¸ LLM-based importance filtering (store only useful memories)
- ğŸ” Semantic retrieval of relevant past interactions
- âš¡ 4-bit quantized Mistral-7B (low VRAM)
- ğŸ›ï¸ Simple Gradio chat UI

---

## ğŸ§© Memory Design

This system uses **three memory layers**:

1. **Summary Memory**
   - Compresses recent conversation
   - Prevents context window overflow

2. **Vector Memory (FAISS)**
   - Stores important interactions only
   - Persists on disk

3. **Importance Filter**
   - The LLM decides whether an interaction is worth saving
   - Avoids memory pollution

---

## ğŸ› ï¸ Tech Stack

- Python
- PyTorch
- Hugging Face Transformers
- Mistral-7B-Instruct
- LangChain (classic + community)
- FAISS
- Sentence-Transformers
- Gradio

---

## ğŸš€ Run on Google Colab

1. Open a new **Google Colab** notebook  
2. Enable GPU:
   `Runtime â†’ Change runtime type â†’ GPU`

3. Install dependencies:

```bash
pip install torch transformers gradio accelerate bitsandbytes \
langchain langchain-community sentence-transformers faiss-cpu
```
4.Run the script

5.Open the generated Gradio share link

---
# ğŸ§  LLM Smart Memory Agent

A minimal **LLM chat system with structured, selective, multi-user memory**.
Built using **Mistral-7B**, **LangChain**, and **FAISS**, and runnable on a
**FREE Google Colab GPU**.

The focus of this project is **memory architecture**, not prompt tricks.

---

## âœ¨ Features

- ğŸ§  Short-term context via `ConversationSummaryMemory`
- ğŸ—‚ï¸ Persistent long-term memory with FAISS
- âš–ï¸ LLM-based importance scoring (0â€“10)
- ğŸ·ï¸ Memory classification (fact / preference / ignore)
- ğŸ‘¥ Multi-user memory isolation
- ğŸ” Semantic memory retrieval
- âš¡ 4-bit quantized Mistral-7B
- ğŸ›ï¸ Simple Gradio chat UI

---

## ğŸ§© Memory Design

Each interaction is processed as follows:

1. Retrieve relevant memories (FAISS)
2. Inject memories into the prompt
3. Generate response using summary memory
4. Ask the LLM:
5. Score importance (0â€“10)
6. Classify memory type:
- `fact`
- `preference`
- `ignore`
7. Persist only useful memories

---

## ğŸ—‚ï¸ Memory Structure

```text
memory_store/
â””â”€â”€ user_id/
  â”œâ”€â”€ facts/
  â””â”€â”€ preferences/
```
---
# ğŸ§  LLM Custom Agent with Decaying Memory & Tools

A **custom-built LLM agent** with manual routing, tool usage, and
**long-term memory with importance decay**, running on a **FREE Google Colab GPU**.

This project avoids LangChain agents and instead implements
**explicit control over routing, memory, and tools**.

---

## âœ¨ Features

- ğŸ¤– Manual router (LLM decides what to do)
- ğŸ§® Calculator tool
- ğŸ§  Short-term context via `ConversationSummaryMemory`
- ğŸ—‚ï¸ Persistent long-term memory with FAISS
- âš–ï¸ LLM-based importance scoring (0â€“10)
- â³ Memory decay over time (exponential decay)
- ğŸ‘¥ Multi-user memory isolation
- âš¡ 4-bit quantized Mistral-7B (NF4)
- ğŸ›ï¸ Gradio chat UI

---

## ğŸ§© Agent Design

This agent does **not** rely on built-in agent abstractions.

Instead, it uses:
- explicit routing logic
- custom tools
- manual memory control

### Routing Logic

For each user message, the LLM chooses one route:

- `calculator` â†’ math expressions
- `memory` â†’ summarize stored memories
- `chat` â†’ normal conversation

---

## ğŸ§  Memory System

Each interaction is processed as follows:

1. Retrieve relevant memories from FAISS
2. Rank memories using **importance Ã— time decay**
3. Inject top memories into the prompt
4. Generate response
5. Score interaction importance (0â€“10)
6. Store only important memories
7. Apply decay over time

### Memory Decay Formula

score = importance * e^(-Î» Ã— age_in_days)


This prevents old or irrelevant memories from dominating context.

---

## ğŸ—‚ï¸ Memory Structure

```text
memory_store/
 â””â”€â”€ user_id/
     â””â”€â”€ facts/
```
Each user has isolated memory

Memories persist across sessions

Stored with metadata (importance + timestamp)
