### mistral-colab-7b
A simple and efficient script to run Mistral-7B-Instruct-v0.2 locally using Hugging Face Transformers. This implementation utilizes 4-bit quantization via bitsandbytes to significantly reduce memory usage, allowing the model to run on consumer-grade GPUs (like Google Colab or local RTX cards).

### Mistral 7B + Gradio running in Google Colab with 4-bit quantization
This code demonstrates how to run the Mistral-7B-Instruct-v0.2 model efficiently on Google Colab using 4-bit quantization. It includes scripts for both a basic text generation example and an interactive Gradio chat interface, allowing users to easily chat with the 7-billion-parameter model on a free GPU without needing high-end hardware.
