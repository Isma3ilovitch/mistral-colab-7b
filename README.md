### Mistral 7B - Local Inference
A simple and efficient script to run Mistral-7B-Instruct-v0.2 locally using Hugging Face Transformers. This implementation utilizes 4-bit quantization via bitsandbytes to significantly reduce memory usage, allowing the model to run on consumer-grade GPUs (like Google Colab or local RTX cards).
