{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q  duckduckgo-search  requests\n",
        "!pip install -qU langchain langchain-community langchain-classic\n",
        "!pip install -qU transformers accelerate bitsandbytes sentencepiece gradio faiss-cpu\n",
        "!pip install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install trafilatura"
      ],
      "metadata": {
        "id": "Nv0F99CiIUmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ca7390-126e-469f-9eb6-59b2b96e4c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import re\n",
        "import torch\n",
        "import gradio as gr\n",
        "import warnings\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import trafilatura\n",
        "\n",
        "from duckduckgo_search import DDGS\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_classic.chains import ConversationChain\n",
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ LOAD LLM (MISTRAL 7B ‚Äì 4BIT)\n",
        "# ======================================================\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(\"üîÑ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"üîÑ Configuring 4-bit quantization...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "# GPU Cleanup\n",
        "import gc\n",
        "print(\"üßπ Cleaning GPU cache...\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"üîÑ Loading model...\")\n",
        "max_memory = {0: \"14GB\", \"cpu\": \"30GB\"}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    max_memory=max_memory,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ LLM loaded\")\n",
        "\n",
        "generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ EMBEDDINGS & VECTOR STORE\n",
        "# ======================================================\n",
        "print(\"üîÑ Loading embeddings...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "BASE_DIR = \"memory_store\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "def load_store(path):\n",
        "    if os.path.exists(path):\n",
        "        return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)\n",
        "    return FAISS.from_texts([\"System: You are a helpful AI assistant.\"], embeddings)\n",
        "\n",
        "def save_store(store, path):\n",
        "    store.save_local(path)\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ IMPORTANCE + DECAY\n",
        "# ======================================================\n",
        "def score_importance(text):\n",
        "    prompt = f\"Score the importance (0-10): {text}\"\n",
        "    out = generation_pipeline(prompt)[0][\"generated_text\"]\n",
        "    match = re.search(r\"\\d+\", out)\n",
        "    return min(10, int(match.group())) if match else 0\n",
        "\n",
        "DECAY_LAMBDA = 0.15\n",
        "def decay_score(importance, timestamp):\n",
        "    return importance * math.exp(-DECAY_LAMBDA * ((time.time() - timestamp) / 86400))\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ TOOLS (FINAL PROMPT FIX)\n",
        "# ======================================================\n",
        "def calculator_tool(expr):\n",
        "    try: return str(eval(expr))\n",
        "    except: return \"Invalid expression\"\n",
        "\n",
        "def date_time_tool(message):\n",
        "    now = datetime.now()\n",
        "    return f\"Today is {now.strftime('%A, %B %d, %Y')}. Time is {now.strftime('%I:%M %p')}.\"\n",
        "\n",
        "# ---------------- WEB SEARCH (TIME-AWARE VERSION) ----------------\n",
        "def web_search_tool(query):\n",
        "    print(f\"üîç Searching for: {query}\")\n",
        "    snippets = []\n",
        "\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            for r in ddgs.text(query, max_results=4):\n",
        "                if r.get('body') and len(r['body']) > 20:\n",
        "                    snippet_text = f\"Source: {r['title']}\\n{r['body']}\"\n",
        "                    snippets.append(snippet_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Search Error: {e}\")\n",
        "\n",
        "    if not snippets:\n",
        "        return \"No relevant search results found.\"\n",
        "\n",
        "    return \"\\n\\n\".join(snippets)\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ ROUTER\n",
        "# ======================================================\n",
        "def decide_tool(message):\n",
        "    message_lower = message.lower()\n",
        "\n",
        "    # 1. Date/Time\n",
        "    time_keywords = [\"today\", \"tomorrow\", \"yesterday\", \"date\", \"time\", \"day\"]\n",
        "    if any(word in message_lower for word in time_keywords): return \"date_time\"\n",
        "\n",
        "    # 2. Calculator\n",
        "    if re.search(r'\\d+\\s*[\\+\\-\\*\\/]\\s*\\d+', message_lower): return \"calculator\"\n",
        "\n",
        "    # 3. Web Search (Broad match)\n",
        "    web_keywords = [\"price\", \"news\", \"now\", \"weather\", \"latest\", \"current\", \"stock\", \"who is\", \"what is\", \"tell me about\", \"films\", \"movies\", \"new\"]\n",
        "    if any(word in message_lower for word in web_keywords): return \"web_search\"\n",
        "\n",
        "    # 4. Memory\n",
        "    memory_keywords = [\"remember\", \"do you know\", \"what do i know\", \"my name\"]\n",
        "    if any(word in message_lower for word in memory_keywords): return \"memory\"\n",
        "\n",
        "    return \"chat\"\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ SESSION BUILDER\n",
        "# ======================================================\n",
        "def get_session(user_id):\n",
        "    user_dir = os.path.join(BASE_DIR, user_id)\n",
        "    os.makedirs(user_dir, exist_ok=True)\n",
        "    store_path = os.path.join(user_dir, \"facts\")\n",
        "    store = load_store(store_path)\n",
        "    memory = ConversationSummaryMemory(llm=llm)\n",
        "    conversation = ConversationChain(llm=llm, memory=memory)\n",
        "    return conversation, store, store_path\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ CHAT LOGIC\n",
        "# ======================================================\n",
        "def chat(message, history, user_id):\n",
        "    conversation, store, store_path = get_session(user_id)\n",
        "    route = decide_tool(message)\n",
        "    print(f\"ü§ñ Route: {route}\")\n",
        "\n",
        "    if route == \"calculator\":\n",
        "        expr_prompt = f\"Extract math expression only: {message}\"\n",
        "        expr = generation_pipeline(expr_prompt)[0][\"generated_text\"]\n",
        "        response = calculator_tool(expr)\n",
        "\n",
        "    elif route == \"date_time\":\n",
        "        response = date_time_tool(message)\n",
        "\n",
        "    elif route == \"memory\":\n",
        "        docs = store.similarity_search(\"\", k=3)\n",
        "        valid_docs = [d.page_content for d in docs if \"System:\" not in d.page_content]\n",
        "        response = \"\\n\".join(valid_docs) or \"No specific memories found yet.\"\n",
        "\n",
        "    elif route == \"web_search\":\n",
        "        try:\n",
        "            web_content = web_search_tool(message)\n",
        "\n",
        "            # FIX: Inject Current Date to prevent hallucinations about old years\n",
        "            current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "You are a helpful assistant answering based on live search snippets.\n",
        "Today's Date: {current_date}\n",
        "\n",
        "Instructions:\n",
        "1. Answer the user's question using the snippets below.\n",
        "2. IGNORE any information that seems old (e.g., years like 2019, 2020).\n",
        "3. DO NOT output multiple conversation turns (User: / Assistant:).\n",
        "4. Provide ONLY the answer, or say \"I found no recent information about this.\"\n",
        "\n",
        "Search Snippets:\n",
        "{web_content}\n",
        "\n",
        "User: {message}\n",
        "Answer:\n",
        "\"\"\"\n",
        "            response = generation_pipeline(prompt)[0][\"generated_text\"].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {e}\")\n",
        "            response = \"I encountered an error while trying to process the web data.\"\n",
        "\n",
        "    else: # Chat\n",
        "        docs = store.similarity_search(message, k=5)\n",
        "        scored = []\n",
        "        for d in docs:\n",
        "            if \"System:\" in d.page_content: continue\n",
        "            meta = d.metadata\n",
        "            score = decay_score(\n",
        "                meta.get(\"importance\", 0),\n",
        "                meta.get(\"timestamp\", time.time())\n",
        "            )\n",
        "            scored.append((score, d.page_content))\n",
        "        scored.sort(reverse=True)\n",
        "        context = \"\\n\".join([c for _, c in scored[:3]])\n",
        "\n",
        "        if context:\n",
        "            response = conversation.predict(input=f\"Relevant memory:\\n{context}\\nUser:\\n{message}\")\n",
        "        else:\n",
        "            response = conversation.predict(input=message)\n",
        "\n",
        "    interaction = f\"User: {message}\\nAssistant: {response}\"\n",
        "\n",
        "    if route not in [\"calculator\", \"date_time\"]:\n",
        "        importance = score_importance(interaction)\n",
        "        if importance >= 6:\n",
        "            store.add_texts([interaction], metadatas=[{\"importance\": importance, \"timestamp\": time.time()}])\n",
        "            save_store(store, store_path)\n",
        "            print(\"üíæ Memory saved\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ GRADIO UI\n",
        "# ======================================================\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    additional_inputs=[gr.Textbox(label=\"User ID\", value=\"user_1\")],\n",
        "    title=\"üß† Time-Aware RAG Agent\",\n",
        "    description=\"Correctly filters out old news and hallucinations.\",\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "vy8imnN54Iy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}