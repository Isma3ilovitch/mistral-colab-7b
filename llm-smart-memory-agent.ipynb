{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community langchain-classic\n",
        "!pip install -qU transformers accelerate bitsandbytes sentencepiece gradio faiss-cpu\n",
        "!pip install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjRNzzsVPbJ1",
        "outputId": "33813f65-dd8f-4d61-effa-1d3224f54494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/157.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/51.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# ======================================================\n",
        "# üî¥ UPDATED IMPORTS (LangChain 1.0+)\n",
        "# ======================================================\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_classic.chains import ConversationChain\n",
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# ======================================================\n",
        "# 1Ô∏è‚É£ Load LLM\n",
        "# ======================================================\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(\"üîÑ Loading Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"üîÑ Loading Model (4-bit)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ LLM loaded\")\n",
        "\n",
        "# We keep 'generation_pipeline' global so the scoring functions can use it directly\n",
        "generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    return_full_text=False  # Prevents repeating the prompt\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
        "\n",
        "# ======================================================\n",
        "# 2Ô∏è‚É£ Embeddings\n",
        "# ======================================================\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# 3Ô∏è‚É£ Memory Helpers (FIXED)\n",
        "# ======================================================\n",
        "BASE_DIR = \"memory_store\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "def load_faiss(path):\n",
        "    if os.path.exists(path):\n",
        "        return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)\n",
        "    # FIX: Use dummy text [\"Initialize\"] to avoid dimension error\n",
        "    return FAISS.from_texts([\"Initialize\"], embeddings)\n",
        "\n",
        "def save_faiss(store, path):\n",
        "    store.save_local(path)\n",
        "\n",
        "# ======================================================\n",
        "# 4Ô∏è‚É£ Importance Scoring (0‚Äì10) (FIXED)\n",
        "# ======================================================\n",
        "def score_importance(text: str) -> int:\n",
        "    prompt = f\"\"\"\n",
        "Score the importance of the following interaction\n",
        "for long-term memory from 0 to 10.\n",
        "\n",
        "Criteria:\n",
        "- Personal preferences\n",
        "- Long-term facts\n",
        "- Goals\n",
        "- Decisions\n",
        "\n",
        "Respond with ONLY a number (0-10).\n",
        "\n",
        "Interaction:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "    # FIX: Use raw pipeline and parse result safely\n",
        "    result_list = generation_pipeline(prompt)\n",
        "    result = result_list[0]['generated_text'].strip()\n",
        "\n",
        "    # Try to find a number in the output (handles \"Score: 7\" or just \"7\")\n",
        "    match = re.search(r'\\d+', result)\n",
        "    if match:\n",
        "        score = int(match.group())\n",
        "        return min(10, max(0, score))\n",
        "    return 0\n",
        "\n",
        "# ======================================================\n",
        "# 5Ô∏è‚É£ Memory Type Classification (FIXED)\n",
        "# ======================================================\n",
        "def classify_memory(text: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "Classify the following interaction.\n",
        "\n",
        "Respond ONLY with one word:\n",
        "- fact\n",
        "- preference\n",
        "- ignore\n",
        "\n",
        "Interaction:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "    # FIX: Use raw pipeline\n",
        "    result_list = generation_pipeline(prompt)\n",
        "    result = result_list[0]['generated_text'].strip().lower()\n",
        "\n",
        "    if \"fact\" in result:\n",
        "        return \"fact\"\n",
        "    if \"preference\" in result:\n",
        "        return \"preference\"\n",
        "    return \"ignore\"\n",
        "\n",
        "# ======================================================\n",
        "# 6Ô∏è‚É£ User Session Builder (FIXED)\n",
        "# ======================================================\n",
        "def get_user_session(user_id: str):\n",
        "    user_dir = os.path.join(BASE_DIR, user_id)\n",
        "    os.makedirs(user_dir, exist_ok=True)\n",
        "\n",
        "    facts_path = os.path.join(user_dir, \"facts\")\n",
        "    prefs_path = os.path.join(user_dir, \"preferences\")\n",
        "\n",
        "    facts_store = load_faiss(facts_path)\n",
        "    prefs_store = load_faiss(prefs_path)\n",
        "\n",
        "    # FIX: Removed return_messages=True to avoid Python object printing\n",
        "    summary_memory = ConversationSummaryMemory(\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    conversation = ConversationChain(\n",
        "        llm=llm,\n",
        "        memory=summary_memory,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return conversation, facts_store, prefs_store, facts_path, prefs_path\n",
        "\n",
        "# ======================================================\n",
        "# 7Ô∏è‚É£ Chat Logic (Elite Memory System) (FIXED)\n",
        "# ======================================================\n",
        "def chat(message, history, user_id):\n",
        "    conversation, facts_store, prefs_store, facts_path, prefs_path = get_user_session(user_id)\n",
        "\n",
        "    # Retrieve memories\n",
        "    facts = facts_store.similarity_search(message, k=2)\n",
        "    prefs = prefs_store.similarity_search(message, k=2)\n",
        "\n",
        "    retrieved = \"\\n\".join(\n",
        "        [d.page_content for d in facts + prefs]\n",
        "    )\n",
        "\n",
        "    augmented_input = f\"\"\"\n",
        "Relevant memories:\n",
        "{retrieved}\n",
        "\n",
        "User message:\n",
        "{message}\n",
        "\"\"\"\n",
        "\n",
        "    response = conversation.predict(input=augmented_input)\n",
        "\n",
        "    # FIX: Trim hallucinated text (stop loops)\n",
        "    for stop_string in [\"\\nUser:\", \"\\nHuman:\", \"\\nAI:\"]:\n",
        "        if stop_string in response:\n",
        "            response = response.split(stop_string)[0].strip()\n",
        "\n",
        "    interaction = f\"User: {message}\\nAssistant: {response}\"\n",
        "\n",
        "    importance = score_importance(interaction)\n",
        "    memory_type = classify_memory(interaction)\n",
        "\n",
        "    # Save memory only if important\n",
        "    if importance >= 6 and memory_type != \"ignore\":\n",
        "        if memory_type == \"fact\":\n",
        "            facts_store.add_texts([interaction])\n",
        "            save_faiss(facts_store, facts_path)\n",
        "        elif memory_type == \"preference\":\n",
        "            prefs_store.add_texts([interaction])\n",
        "            save_faiss(prefs_store, prefs_path)\n",
        "\n",
        "        print(f\"üíæ Saved {memory_type} (Score: {importance}) for {user_id}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# ======================================================\n",
        "# 8Ô∏è‚É£ Gradio UI (Multi-User Ready) (FIXED)\n",
        "# ======================================================\n",
        "# FIX: Removed theme=\"soft\" to prevent TypeError in new Gradio\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(label=\"User ID\", value=\"user_1\")\n",
        "    ],\n",
        "    title=\"üß† Smart LLM with Multi-User Memory\",\n",
        "    description=\"Importance scoring + memory types + FAISS (FREE Colab)\",\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1e5718610d554a91aeaaa9b18fbec303",
            "9027a31179cd44ac9fb9b26e60df4ed8",
            "fa4b6cb35d0f4fbab274f8e9c8a4c411",
            "7784bffab9dc4af7b70168f05126f248",
            "d4907b4e728641d1a768894c9ced7551",
            "c8925f7219d34d19864b2251133128fa",
            "0588f923fcac43a0bbc623eb4ea6570a",
            "f5ca224da32c4021b11ceb2f74b79fe6",
            "1aae0c46ad7a4707b3a579be4ccd984d",
            "4a273168fde844dfad1daa38c3f151cf",
            "3d22d931735048bea3eea0d21fa8f50f",
            "afada038644b4214a30ce12abe9be6bd",
            "47735298bac243d38ba6a6438e3a72bf",
            "f10ae3544a3e440f93224036b98f0ea6",
            "f9ff6b557aa64a31be42bf7cbc4367bd",
            "e034d9f91d81459ab0bcbd500bea2a28",
            "71baac152ea84e71a0adde8d24a3eb6f",
            "ddd86e599f1b4d369952743b19f73c4b",
            "1d8b11f33526438c85663cbf8c466a8b",
            "9793726d3498471aaaf52a560e257d74",
            "5e3fe80d02904b3fb7568e930bef88f2",
            "6c7e2c0a5ad74f9c8846a4b752fa106a",
            "0edc98df7d5846eeb815f0e66de3918d",
            "e52ca96980c448ffa2f5a8d9a4f918f1",
            "44f9991d0e8645e1b58e807d9a79e8d8",
            "9cd23744977047a98e63400dd1c85f00",
            "a4ed3e2a1a0d4424afda1689eb2840d8",
            "0176f62d99374d81920d5909a28f845c",
            "46b0f91980d34a9da06d03f4903435e2",
            "75826afbb3ed4f12970964f6ae8d9c81",
            "249f89dfa90848068e055a0d6689b617",
            "b7a10ed816c9410fb540ab28acd522db",
            "a0020f0df65e4bc5bae15bf9c0b79337",
            "55dc59a918744e9cb76471a86a3e8dee",
            "0055317dbf1f4dea8cf0d14889ea3fd0",
            "f399313f17c147b8884e492265026573",
            "f28927fe1e79406f8d6c6cb573f642dd",
            "463e43d1f6fa40728e1de055843573cd",
            "a97f8f1a65ec4f85ad067b5b0f199ba5",
            "8c9a151b8a1a4bcc9df1497a82d75c06",
            "3c8cd129d3d643f8b9d77ff1a28983f2",
            "2233d2e57d724661921adebe18e77dff",
            "d228c32e4373498ca16907c212977440",
            "81ee93a6ed584d75a1a6d3231afa87e7",
            "b56f6642ff104fd9a534540a2e379aef",
            "b18a1a3f78d94bf08a0ec7c2071172a4",
            "e29e2b3afaef4296ac9f767c46ab8140",
            "815df583493d470d97c0f1ccd24dffe9",
            "d0e34b92b9b049d391f1306da3eb8e10",
            "d90e54359ecc4f7b9c22fa9d3e7c7de7",
            "f87575b1a5624016973d81e1e80aa00e",
            "bcb3f7961fd24c90b6cdfe191b211a1e",
            "92888b5065304698a4d2d2a9573b4c2c",
            "fd4aa6b3e8424e2aa70a8bc9701be634",
            "1c2af69ec2cf4bf6b4d1dd4a74517682",
            "c2ca193cbd8f470ea0228f656b6aa03c",
            "f8132b633a7242c39553c77a3bfe048d",
            "5183f71bf07240e9b43af35db71d4a92",
            "f19ae53ee1bc4cffbc11b0a9235b25f1",
            "9c7d5e9cd94b48bf8a23da13ed96192d",
            "0c0ed32981754813993d92da0441b487",
            "c08ad70b95ea45ccb2cb35ce673fea01",
            "662b45472fb04f3f861f1a0f88652dbc",
            "d8ac0552684e4d3180b0f6abb378c8f4",
            "18bd82c15d7546298764830f2bbf55d0",
            "1476c901b44b4d5f95ef30808fad3bff",
            "4ef4ed9ab5894f668241102cd5e977f9",
            "11b6c15e631140fd925d99ed017d9334",
            "a8bfab07d10844f3a5c327a11c4811d1",
            "97dd961306cd4a4f9261b0f96281e712",
            "6762214320c549a39492cb8c2e50c975",
            "cd768aca81f0484aa46988cf2b4211fb",
            "fe3d3b57da214870834dca076839f3df",
            "030e556a113b40daa8cd2f010149f31c",
            "6630a3cebdec43fca833390318da2675",
            "bba4b5ab308549f28b25068f59d7964b",
            "36449c607e6945f2b65e4d1b5117f9e3",
            "1435faa652494f1ebfabcdeae9c64b40",
            "48086ff9f5e74feaaa611979c63ed0ef",
            "0ed90da79dcf412fbf1dbaa36d11ae94",
            "aaf7fee672124a82a181d71b776089d4",
            "81daf6214cf14fe68bc0fc4db1fbe64c",
            "3ba662bece6d4ef3b281c8d7d1cf3ee7",
            "68b431d221b7411486803560cec74227",
            "2d3a99fe1b7e4d969b9c68853fcecd87",
            "6cff3a41bf7249bdb6453131541ec65f",
            "401b3b682b294bccbd1e9ac1d415c015",
            "3c637d984aea4f409f75768bbaaef7e3",
            "ff44580951774e32844898f81d077ea6",
            "fe471dc2d8c745ab99f30753c9dd1730",
            "0c396c6abdf1484cb722811501fff813",
            "18f7ef5b07db4512abccccb2f02cc9b4",
            "ad142d16b6b047168dd6d492c790635b",
            "6208b5ea330341ce85ff76f73f53bda3",
            "cd9a61d4282c44d4a9a7328cd1166ba9",
            "e5660cf7679347979596b9e06327f6e8",
            "b286601acbff47d89e41c758ffdc3e98",
            "27d2eb1a0ba948e7af3b0634fecbf4e1",
            "c9c92a3c1aa1451baafd32fb90a803e5",
            "69cddca4b84b440b84552361500d2fc7",
            "fa0818277d694539a8f097466a4f030f",
            "a2aab818962f45dcbae5fcc110240578",
            "a6eb84fa2bf544ed9e8d1d721ca43cd1",
            "534dcfe8d4514d1db1b89972f018dfc9",
            "95692f276bde48529464f8997997c957",
            "53d1eac4020143e19b5f7e1b370a743b",
            "ba44c39aa6e54c2a9b9a4857e88a74dc",
            "8b619932574c4d03b80a58b84f5805cb",
            "447e00e8719b438a9f398759a613bbc6",
            "3b18aaa92e0341fc912da75eddc6fd22",
            "9955764a951946dcabf9aa1ebbe4aa35",
            "419568a9a7e24389bf81927f07daec69",
            "fd20c968228d42d5966afd716cf5544d",
            "1992e671ee794279b9bddddeece72f26",
            "dfef82a3a61f4b25ac948f019a9ab3c9",
            "50d41d65047943ef81a08b826a9cd914",
            "b438bafa7d2944ffa379523cd3ca4ca7",
            "9cfd817c766c4e27af16bd9f19df403e",
            "42dc0e2bfb6d48f98855c85f2e14899e",
            "51957a5696374e8a80a2f9ff2dc17fa4",
            "de9dc95af1624084b490637e18127d7f",
            "1b5b1f1a626f42068f243b8613b37c60",
            "c07ff8e2b7c24e30876dd9f34245c9b5",
            "c7edbb896a2c4bd48b3e96d5c7a6e6bb",
            "a3082f20eeff424fb6a9b4c1e189870b",
            "c45bdfc64aec40eab7c3295d81008522",
            "9a7b48a0fc2e4f4a95c482d5ea684e93",
            "5d63eb220ed14e1da669e8abeb5e06b6",
            "febaa843336245ceb4069d152ffb3760",
            "ca1e8cf647fa4d4380b9b6e8c5d5f303",
            "4baaa2a5c8d742ecb59ee5a2bb481404",
            "d29ccd75cb374b61892f2d810df25d35",
            "09c1a44a0dc54d9bb55687531705c011",
            "cffa35fab2f144e087e0990c9ff250fb",
            "0a72013fd7e04f648e39529fa644aee9",
            "2057a64649c345d694da24c17099468a",
            "789d6cf04e374049b47e9451206494ae",
            "b2ef295fb60f4b6b84f233caa1b72260",
            "40cd758c2cb04e8a9ac9df303d5eabf2",
            "592d34c9e8734e2cbccbe2bdb86f77e2",
            "34b68bb35a464bf6aa30cb17380b52be",
            "ba6307048fb34a21a524d7de2dfa4fc4",
            "0c2878b13f6a4adfbaabb8aa3c8dffb3",
            "53033730190c45f4be842aff300ee64f",
            "898c1ed443f3443bb4947eef0e1db3c3",
            "617ecb56e8e542b4a7ca07ddf6b90b39",
            "760ff5f6453b44b8b7da1034b358fe9a",
            "4c4e367d6c4a4a21a52f7cfe2d312feb",
            "0445dcc5baae45b79a1014c28901485d",
            "efb9e72602ba4e5ab1cbeaa46c991279",
            "aa1a8d95721143069b6364c9c938ee4c",
            "ddaa9b91ffbf4af88891b2f8d3e00c40",
            "d453ab0df70442eb82fb8ef47cf48cf0",
            "7ab9d1ebacae4fdba6aeaa629b35e304",
            "cbdd9c86aa3f4e3caf5b724c57454260",
            "4b96ca1af4ea43b28e2ade96e11dba06",
            "547e5d3f51ef4a8d81eef2e469dbe67a",
            "3969f9a59a104905b2beeab0dbfbbabe",
            "71beac37053a41aca993c9fec91e4f63",
            "37a1d326366e471c8c3c9abf8e3b27e7",
            "bafec72ecb2d4e9890ab31249bacdd8c",
            "e80f4eca6601425e9caab3885b0839c9",
            "776f1083bc3f474e9cdf77a2dfa68d0d",
            "31d3870f29854fda98409cddcc511c50",
            "7b67934128724be0aaffe9d254c519d0",
            "a2dd01ceaf77409fac152978aad774b2",
            "c436d42138164f408744b59deb733b28",
            "811f114b1f7d4557a88a17b198b90aa0",
            "27d2c5bcfef6497c9195bd8e94718a60",
            "2681c1915ca34c90bf640017dc2c16c8",
            "4883d00f64704169ad41b64d0dade257",
            "7f2a17a5f11d46b7b25d8f6ba4c0891f",
            "3241527f8cfa445d9da9e54763efc41e",
            "69ee1211515d4abda4cb1cc501015f46",
            "43f0d72c4b2a4df89b56a402600d715d",
            "54539ebaa9ba4458b5ecca6447f3f4c8",
            "aba374ca478b424ca888d454273db035",
            "136f95b129334bef8a0de3583e998705",
            "b21f74a7a068411da2a2ff76b7db5dca",
            "aa249576f8f5476fb81c58dd3b56dae2",
            "4210f5579d6148508e3e12240c2dfcaa",
            "6b4a9a620e3b4139965bebec0a740892",
            "fc99db97b8184ca091f5dc62d59e3c2b",
            "4092e725a31e4779a1b7a01f94c6f9ae",
            "8c06a16e2858415bba42cfbe73cb474d",
            "7fe3fa3d0e6146d5ab79e45581e3531d",
            "95ff8e1fa4ef4408a0bae54ca2058962",
            "a8cce3611018481fb5f870f8d73726b0",
            "57e26f9152614183b8010c25ad56de07",
            "0216b25643e3430494a88ff1c79fc424",
            "11409040aa564691b7eba377658845a8",
            "e35d34c50b694eb0be8974d4a411ea47",
            "74b4b6596c3f4e48be3f6d7f9f28fcc2",
            "025b69c77f4c433cabf0bb709326bedd",
            "0af540985f634277b2c70be05fe020df",
            "d0edfad543dd45608746c5d2d31b3aa3",
            "a5a29febc54c43d48d0c8b038ad7b788",
            "ab1f71c80a4c441d9242c3aae9e140ea",
            "0bb39c17cb8441ceb6b56f408d851f19",
            "cd1e7079504041c0b7922ed9b61bfa09",
            "1a2ac58fb2a24b678a61a307d2faed67",
            "0e1b47e5cb6649bf9e9cfebd28ca347c",
            "4af902f9fb404f99a6246bcab5ea9008",
            "5f1c30b4c0084d588f375011eb5e48c4",
            "f2a3b1c3628446f39aec917c081b8257",
            "0db3d081765c40b3862e672f9014b013",
            "4991c07235e043eabf6468e78449934a",
            "0f0e17f5592c4f4093c05d425479d782",
            "4c39d8f599dc4c5192f13ba5de5a881d",
            "33a8909c4f2049158e836357b3e2c95d",
            "d1a9f1265462402d9e18fd52d4d1caa1",
            "50c9c5c35d33417b84534577dae716a2",
            "339643c4b8be4ccc9e1c224caeeaadd6",
            "5e7fd7afa0d1481d918437b8913f6ff5",
            "244a584e6e1741bab865aba8e9be8205",
            "aaca47d667dd439b9a12def079a1e415",
            "c9c76a9814c3412593ce6f4aae7de4e4",
            "3e23a4a8d3e6491a8fadbe6e2d942f88",
            "b5a7fe94cc72418381707d0dbde7d695",
            "f97afe5366cd44c88858b25950e29e4a",
            "bd0f5488ebab455087f605b9a2c0df67",
            "3a744edf19574b1c8c34357a23fcac26",
            "5e39b48147684a2d83b33f377c9755fb",
            "f36bbbcb85ab4b848daff4b345ba739d",
            "e97fb563c7ec4900b31019979e5c094c",
            "9f7f8e564b6f4073a5fe6812dbbef00b",
            "7734292ba36d403a9dcf0f454b7c16c1",
            "90a8fbee92e8460f9c4abed4102eb497",
            "c028d2287d9d48128bba9be795c01e9e",
            "eed5406f0f064b45b1064f27d131043e",
            "b7f82a75f4ee44f1b54e175e43e557c8",
            "a25a4beec0b94c988ea5f45b15032a1f",
            "6bf6c085f98b48209eba367433173143",
            "7ae36fe9b1734294b680bb9f36f2fefe",
            "176e7b7917cf44c9b021c24138e361a7",
            "938cde432bce4128b0e654c65632abfd",
            "8d2d8dd65df74e08ac3046b0ebf7ff1c",
            "5910a84bad6f4838bcb2120ce37d06ef",
            "924abf4dc9be4e258c53adc5a0cc91a6",
            "332cc424bd224f159ff3c80f333e3236",
            "fb6fc703ae194314ab6017f24429e8f6",
            "2019549adda74827997e40d87e35a718",
            "d8a20a97c5ce4a3ead5d7282f9aec8ea",
            "f6698a39977a4ff19026281f32f860cc",
            "b186407b73b144dba59685927293f760",
            "8cc6fc6ab2464ddb9b98a8162c9ca356",
            "e438e2624d674875babea08729d1df37",
            "44984b2e681e4e6685ad4d09f68bccdb",
            "95968a59196a450694cd044d7cb7cd34",
            "2fbad11463ca4b9da38e0bbeec7afc09",
            "9ef86f9e24d646119cd5e1512cd86287",
            "d26f2888d2be4f05a3a82f1f0b71f707",
            "496dc96041b14c0bbffb56741d6bf550"
          ]
        },
        "id": "DF__I0mtQxhI",
        "outputId": "233006da-5fa3-4a06-d375-3c85a8e99959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading Tokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e5718610d554a91aeaaa9b18fbec303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afada038644b4214a30ce12abe9be6bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0edc98df7d5846eeb815f0e66de3918d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55dc59a918744e9cb76471a86a3e8dee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading Model (4-bit)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b56f6642ff104fd9a534540a2e379aef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2ca193cbd8f470ea0228f656b6aa03c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ef4ed9ab5894f668241102cd5e977f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1435faa652494f1ebfabcdeae9c64b40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff44580951774e32844898f81d077ea6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69cddca4b84b440b84552361500d2fc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9955764a951946dcabf9aa1ebbe4aa35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b5b1f1a626f42068f243b8613b37c60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LLM loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1094727175.py:53: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
            "/tmp/ipython-input-1094727175.py:58: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09c1a44a0dc54d9bb55687531705c011"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53033730190c45f4be842aff300ee64f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbdd9c86aa3f4e3caf5b724c57454260"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2dd01ceaf77409fac152978aad774b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aba374ca478b424ca888d454273db035"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8cce3611018481fb5f870f8d73726b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bb39c17cb8441ceb6b56f408d851f19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33a8909c4f2049158e836357b3e2c95d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd0f5488ebab455087f605b9a2c0df67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a25a4beec0b94c988ea5f45b15032a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8a20a97c5ce4a3ead5d7282f9aec8ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://02c9f0d8a9701209d0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://02c9f0d8a9701209d0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}
